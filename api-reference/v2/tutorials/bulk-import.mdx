---
title: Bulk Import
description: 'Importing large datasets into a Big Table'
---

When importing large datasets into a Big Table it is necessary to upload the data in stages. This eliminates the volatility of long-running requests and allows for higher performance by parallelizing the upload process. This process is called "stashing".

<Tip>
  Please read our [introduction to stashing](/api-reference/v2/stashing/introduction) to understand how stashing works before looking at bulk imports specifically.
</Tip>


## Overview

The general sequence of operations for a large import is:

<Steps>
  <Step title="Create stash identifier">
    Create a stable stash ID that will associate all subsequent data uploads.
  </Step>
  <Step title="Upload data subsets">
    Upload data in stages of 500 to 1,000 rows, using the stash ID from step one.
  </Step>
  <Step title="Finalize import">
    Once all data has been uploaded, use the stash ID to import the full dataset as a single atomic table operation.
  </Step>
</Steps>

## Create Stash ID

To simplify the coordination, parallelization, and idempotency of the upload process, the stash ID is a value that you create from the relevant information of your domain.

For instance, a daily import process might have a stash ID of `20240501-import`. Or, an import specific to a single customer might have a stash ID of `customer-381-import`.

<Warning>
  The stashID must be in the format of a UUID, e.g., `"123e4567-e89b-12d3-a456-426655440000"`. This is a known issue and will be fixed in a future release.
</Warning>

You are responsible for ensuring that the stash ID is unique and stable across associated uploads.

## Upload Data

Once you have a stable stash ID, you can use the [stash data endpoint](/api-reference/v2/stashing/post-stashes-serial) to upload the data in stages.

Upload stages can be run in parallel to speed up the upload of large dataset, just be sure to use the same stash ID across uploads to ensure the final data set is complete.

As an example, the following [stash](/api-reference/v2/stashing/post-stashes-serial) requests will create a final dataset consisting of the two rows identified by the stash ID `20240501-import`.

<Tabs>
    <Tab title="POST /stashes/20240501-import/1">
        ```json
        [
            {
                "Name": "Alex",
                "Age": 30,
                "Birthday": "2024-07-03T10:24:08.285Z"
            }
        ]
        ```
    </Tab>
    <Tab title="POST /stashes/20240501-import/2">
        ```json
        [
            {
                "Name": "TomTom",
                "Age": 26,
                "Birthday": "2024-07-03T10:27:43.265Z"
            }
        ]
        ```
    </Tab>
</Tabs>

<Note>The trailing parameters of `1` and `2` in the request path are the serial IDs, which distinguish and order the two uploads within the stash.</Note>

## Finalize Import

Once all the data to be imported has been uploaded, you can use the stash ID in one of Glide API's table endpoints to import the full dataset in a single atomic operation.

### Create New Table

To create a table with the data of the stash ID `20240501-import` you can use the [create table](/api-reference/v2/tables/post-tables) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "name": "New Table",
    "schema": {
        "columns": [ ... ]
    },
    "rows": {
        "$stashID": "20240501-import"
    }
}
```

### Add Rows to Table

To add data to an existing table you can use the [add rows to table](/api-reference/v2/tables/post-table-rows) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "$stashID": "20240501-import"
}
```

### Overwrite Table

To reset an existing table's data you can use the [overwrite table](/api-reference/v2/tables/put-tables) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "$stashID": "20240501-import"
}
```