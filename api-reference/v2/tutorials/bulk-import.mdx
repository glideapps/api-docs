---
title: Bulk Import
description: 'Importing large datasets into a Big Table'
---

When importing large datasets into a Big Table it is necessary to upload the data in stages. This eliminates the volatility of long-running requests and allows for higher performance by parallelizing the upload process. This process is called "stashing".

<Tip>
  Please read our [introduction to stashing](/api-reference/v2/stashing/introduction) to understand how stashing works before looking at bulk imports specifically.
</Tip>


## Overview

The general sequence of operations for a large import is:

<Steps>
  <Step title="Create stash identifier">
    Create a stable stash ID that will associate all subsequent data uploads.
  </Step>
  <Step title="Upload data subsets">
    Upload data in stages of 500 to 1,000 rows, using the stash ID from step one.
  </Step>
  <Step title="Finalize import">
    Once all data has been uploaded, use the stash ID to import the full dataset as a single atomic table operation.
  </Step>
</Steps>

## Create Stash ID

To simplify the coordination, parallelization, and idempotency of the upload process, the stash ID is a value that you create from the relevant information of your domain.

For instance, a daily import process might have a stash ID of `20240501-import`. Or, an import specific to a single customer might have a stash ID of `customer-381-import`.

You are responsible for ensuring that the stash ID is unique and stable across associated uploads.

## Upload Data

Once you have a stable stash ID, you can use the [stash data endpoint](/api-reference/v2/stashing/put-stashes-serial) to upload the data in chunks.

Chunks can be sent in parallel to speed up the upload of large datasets. Use the same stash ID across uploads to ensure the final data set is complete, and use the serial to control the order of the chunks within the stash.

As an example, the following [stash](/api-reference/v2/stashing/put-stashes-serial) requests will create a final dataset consisting of the two rows identified by the stash ID `20240501-import`. The trailing parameters of `1` and `2` in the request path are the serial IDs. The data in serial `1` will come first in the stash, and the data in serial `2` will come second, even if the requests are processed in a different order.

<Tabs>
    <Tab title="PUT /stashes/20240501-import/1">
        ```json
        [
            {
                "Name": "Alex",
                "Age": 30,
                "Birthday": "2024-07-03T10:24:08.285Z"
            },
        ]
        ```
    </Tab>
    <Tab title="PUT /stashes/20240501-import/2">
        ```json
        [
            {
                "Name": "TomTom",
                "Age": 26,
                "Birthday": "2024-07-03T10:27:43.265Z"
            }
        ]
        ```
    </Tab>
</Tabs>

<Note>The above is just an example. In practice, you should include more than one row per stash chunk, and if your complete dataset is only 2 rows, you do not need to use stashing at all. See [Limits](/api-reference/v2/general/limits) for guidance.</Note>

## Finalize Import

Once all the data to be imported has been uploaded, you can use the stash ID in one of Glide API's table endpoints to import the full dataset in a single atomic operation.

### Create New Table

To create a table with the data of the stash ID `20240501-import` you can use the [create table](/api-reference/v2/tables/post-tables) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "name": "New Table",
    "schema": {
        "columns": [ ... ]
    },
    "rows": {
        "$stashID": "20240501-import"
    }
}
```

The endpoint will return a list of row IDs in the same order as the rows in your stashed data, preserving the order defined by the serial numbers in your stash uploads.

### Add Rows to Table

To add data to an existing table you can use the [add rows to table](/api-reference/v2/tables/post-table-rows) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "$stashID": "20240501-import"
}
```

The endpoint will return a list of row IDs in the same order as the rows in your stashed data, preserving the order defined by the serial numbers in your stash uploads.

### Overwrite Table

To reset an existing table's data you can use the [overwrite table](/api-reference/v2/tables/put-tables) endpoint with the `stashID` reference of `20240501-import` instead of the actual row values.

```json
{
    "$stashID": "20240501-import"
}
```