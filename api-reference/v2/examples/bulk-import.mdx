---
title: Bulk Import
description: 'Importing large datasets into a Glide Big Table'
---

When importing large datasets into a Glide Big Table it is necessary to upload the data in stages. This eliminates the volatility of long-running requests and allows for higher performance by parallelizing the upload process.

## Overview

The general sequence of operations for a large import is:

<Steps>
  <Step title="Create import identifier">
    Create a stable import ID that will associate all subsequent data uploads.
  </Step>
  <Step title="Upload data in chunks">
    Upload data in chunks of 1,000 to 10,000 rows, using the import ID from step one.
  </Step>
  <Step title="Finalize import">
    Once all data has been uploaded, use the import ID to import the full dataset as a single atomic table operation.
  </Step>
</Steps>

## Create Import Identifier

To simplify the coordination, parallelization, and idempotency of the upload process, the import ID is a value that you create from the relevant information of your domain.

For instance, a daily import process might have an import ID of `20240501-import`. Or, an import specific to a single customer might have an import ID of `customer-381-import`.

You are responsible for ensuring that the import ID is unique and stable across associated uploads.

## Upload Data in Chunks

Once you have a stable import ID, you can use the `POST /v2/imports/` endpoint to upload the data in chunks.

Upload chunks can be run in parallel to speed up the upload of large dataset, just be sure to use the same importID across uploads to ensure the final data set is complete.

As an example, the following upload request bodies will create a dataset of two rows total within the import ID `20240501-import`.

```json
{
    "data": {
        "importID": "20240501-import",
        "rows": [
            {
                "Name": "Alex",
                "Age": 30,
                "Birthday": "2024-07-03T10:24:08.285Z"
            }
        ]
    }
}
```

```json
{
    "data": {
        "importID": "20240501-import",
        "rows": [
            {
                "Name": "TomTom",
                "Age": 26,
                "Birthday": "2024-07-03T10:27:43.265Z"
            }
        ]
    }
}
```

## Finalize Import

Once all the data to be imported has been uploaded, you can use the import ID in one of Glide API's table endpoints to import the full dataset in a single atomic operation.

For instance...

### Append to Table

To append all the data of an import to an existing table you can use the `POST /tables/{{tableID}}/rows` endpoint.

### Overwrite Table

To overwrite all data of a table with the data of an import you can use the `PUT /tables/{{tableID}}` endpoint.

### Create New Table

To create a new table with the data of an import you can use the `POST /tables/` endpoint.